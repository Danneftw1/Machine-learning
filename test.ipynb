{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Read from DynamoDB\n",
    "    table_name = os.environ[\"BIKE_TABLE_NAME\"]\n",
    "    table = dynamodb.Table(table_name)\n",
    "\n",
    "    # Calculate timestamp for two weeks ago\n",
    "    two_weeks_ago = datetime.now() - timedelta(days=14)  # \"from today and 14 days back\"\n",
    "\n",
    "    # kan testa att ta bort hh:mm:ss\n",
    "\n",
    "    # filtering for records where timestamp is greater than the calculated 'two_weeks_ago'\n",
    "    response = table.scan(\n",
    "        FilterExpression=\"#ts > :two_weeks_ago\",\n",
    "        ExpressionAttributeNames={\"#ts\": \"timestamp\"},\n",
    "        ExpressionAttributeValues={\":two_weeks_ago\": two_weeks_ago.isoformat()},\n",
    "    )\n",
    "    # checks if there are any items in the response\n",
    "    items = response[\"Items\"] if \"Items\" in response and response[\"Items\"] else []\n",
    "\n",
    "    # if no items are found\n",
    "    if not items:\n",
    "        return {\"message\": \"No items found in DynamoDB for the last two weeks.\"}\n",
    "\n",
    "    # Convert to CSV format\n",
    "    csv_file = StringIO()  # behaves like a file but data is stored in memory\n",
    "    csv_writer = csv.DictWriter(\n",
    "        csv_file, fieldnames=items[0].keys()\n",
    "    )  # assuming all items have the same keys\n",
    "    csv_writer.writeheader()  # writes the header (column names)\n",
    "    for item in items:  # loops through the items\n",
    "        csv_writer.writerow(item)  # writes a row for each loop\n",
    "    csv_data = csv_file.getvalue()\n",
    "    csv_file.close()\n",
    "\n",
    "    # Write to S3\n",
    "    bucket_name = os.environ[\"S3_BUCKET_NAME\"]\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d%H%M%S\")  #\n",
    "    s3_key = f\"testdata_{timestamp_str}.csv\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_key, Body=csv_data)\n",
    "\n",
    "    return {\n",
    "        \"message\": f\"Data from the last two weeks saved to s3://{bucket_name}/{s3_key}\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aws_cdk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Github\\Machine-learning\\test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Github/Machine-learning/test.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39maws_cdk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Github/Machine-learning/test.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mboto3\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Github/Machine-learning/test.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbotocore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m ClientError\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'aws_cdk'"
     ]
    }
   ],
   "source": [
    "import aws_cdk\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from aws_cdk import (\n",
    "    aws_dynamodb,\n",
    "    aws_lambda,\n",
    "    aws_iam,\n",
    "    Duration,\n",
    "    aws_events,\n",
    "    aws_events_targets as targets,\n",
    "    aws_s3 as s3,\n",
    ")\n",
    "\n",
    "\n",
    "# VIKTIGT: OBJEKT BLIR INSTATIERADE I BÖRJAN AV KODEN\n",
    "# DET HÄR FÅR DET ATT SE UT SOM ATT EN VARIABEL BLIR KALLAD FÖRE DEN ÄR SKAPAD\n",
    "# Funktioner i classer kallar för metoder\n",
    "\n",
    "from constructs import Construct\n",
    "import os\n",
    "\n",
    "\n",
    "# Class definition and inheritance\n",
    "class DataFetchAndSave(Construct):  # inherits from Construct\n",
    "    def __init__(  # initialize the object and expect the following parameters\n",
    "        self, scope: Construct, id_: str, stage_name: str, service_config: dict\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            scope, id_\n",
    "        )  # Calls the parent class' constructor to initialize it\n",
    "\n",
    "        # extracts the service name from the configuration\n",
    "        service_short_name = service_config[\"service\"][\"service_short_name\"]\n",
    "        service_name = service_short_name\n",
    "\n",
    "        # Formats the lambda name and role name\n",
    "        lambda_name = f\"{stage_name}-{service_short_name}-data-fetch-and-save\"\n",
    "        lambda_role_name = f\"{lambda_name}-role\"\n",
    "        # \"lambda_role\" calls to get the role object for the lambda\n",
    "        lambda_role = self._build_lambda_role(lambda_role_name)\n",
    "\n",
    "        # environment variables for the lambda\n",
    "        # configure the enviroment differently based on the stage or service\n",
    "        # this is done by passing in the stage name and service name, which will change based on if you're deploying to dev or prod\n",
    "        env_vars = self._create_env_vars(\n",
    "            stage_name=stage_name,\n",
    "            service_name=service_name,\n",
    "            service_short_name=service_short_name,\n",
    "        )\n",
    "\n",
    "        bucket_name = env_vars[\"S3_BUCKET_NAME\"]\n",
    "        unique_id = f\"BucketConstruct-{bucket_name}\"\n",
    "\n",
    "        s3.Bucket(self, id=unique_id, bucket_name=bucket_name)\n",
    "\n",
    "        # builds lambda function - passes in the necessary configuration\n",
    "        self.data_fetch_and_save_lambda = self._build_lambda(\n",
    "            lambda_name=lambda_name,\n",
    "            stage_name=stage_name,\n",
    "            env_vars=env_vars,\n",
    "            lambda_role=lambda_role,\n",
    "        )\n",
    "\n",
    "        # CloudWatch Event to trigger the lambda function every two weeks\n",
    "        rule = aws_events.Rule(\n",
    "            self, \"Rule\", schedule=aws_events.Schedule.rate(Duration.days(14))\n",
    "        )\n",
    "        rule.add_target(targets.LambdaFunction(self.data_fetch_and_save_lambda))\n",
    "\n",
    "        # Adding permissions to access the DynamoDB table\n",
    "        lambda_role.add_to_policy(\n",
    "            aws_iam.PolicyStatement(\n",
    "                actions=[\"dynamodb:Scan\", \"dynamodb:GetItem\"],\n",
    "                resources=[\n",
    "                    \"arn:aws:dynamodb:eu-north-1:796717305864:table/Cyrille-dscrap-bike-data-table\"\n",
    "                ],  # DynamoDB table ARN\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # S3 permissions here\n",
    "        lambda_role.add_to_policy(\n",
    "            aws_iam.PolicyStatement(\n",
    "                actions=[\"s3:PutObject\"],\n",
    "                resources=[f\"arn:aws:s3:::{bucket_name}\"],  # S3 bucket ARN\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # def _create_s3_bucket(self, bucket_name: str) -> (s3.Bucket, str):\n",
    "    #     unique_id = f\"BucketConstruct-{bucket_name}\"\n",
    "    #     bucket = s3.Bucket(\n",
    "    #         self,\n",
    "    #         unique_id,\n",
    "    #         bucket_name=bucket_name,\n",
    "    #     )\n",
    "    #     return bucket\n",
    "\n",
    "    # creates an AWS Lambda function\n",
    "    def _build_lambda(\n",
    "        self,\n",
    "        lambda_name: str,  # name of the lambda\n",
    "        stage_name: str,  # Sets stage name, so i we know if its dev or prod etc\n",
    "        env_vars: dict,  # environment variables\n",
    "        lambda_role: aws_iam.Role,  # Role - for permissions\n",
    "        cwd: str = os.getcwd(),  # current working directory\n",
    "    ):\n",
    "        lambda_function = aws_lambda.Function(\n",
    "            self,\n",
    "            lambda_name,  # name that has logicalID in template.json\n",
    "            function_name=lambda_name,\n",
    "            runtime=aws_lambda.Runtime.PYTHON_3_10,  # runtime setting - python 3.10\n",
    "            environment=env_vars,\n",
    "            code=aws_lambda.Code.from_asset(  # where the code for lambda function exists\n",
    "                os.path.join(cwd, \"bike_data_scraper/handlers\")\n",
    "            ),\n",
    "            handler=\"data_fetch_and_save_lambda.lambda_handler\",\n",
    "            tracing=aws_lambda.Tracing.ACTIVE,\n",
    "            retry_attempts=2,\n",
    "            timeout=Duration.seconds(80),  # time before lambda times out\n",
    "            memory_size=128,  # default memory size\n",
    "            role=lambda_role,\n",
    "        )\n",
    "        return lambda_function\n",
    "\n",
    "    def _build_lambda_role(self, role_name: str) -> aws_iam.Role:\n",
    "        lambda_role = aws_iam.Role(\n",
    "            self,\n",
    "            role_name,\n",
    "            assumed_by=aws_iam.ServicePrincipal(\"lambda.amazonaws.com\"),\n",
    "            managed_policies=[\n",
    "                aws_iam.ManagedPolicy.from_aws_managed_policy_name(\n",
    "                    managed_policy_name=\"service-role/AWSLambdaBasicExecutionRole\"\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "        return lambda_role\n",
    "\n",
    "    # environment variables for the lambda\n",
    "    def _create_env_vars(\n",
    "        self, stage_name: str, service_name: str, service_short_name: str\n",
    "    ) -> dict:\n",
    "        return {\n",
    "            \"STAGE_NAME\": stage_name,\n",
    "            \"S3_BUCKET_NAME\": f\"{stage_name}-{service_short_name}-processed-bike-data\",\n",
    "            \"BIKE_TABLE_NAME\": \"Cyrille-dscrap-bike-data-table\",\n",
    "            \"LOG_LEVEL\": \"DEBUG\",\n",
    "            \"SERVICE_NAME\": service_name,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_cdk import Stack\n",
    "from constructs import Construct\n",
    "\n",
    "from infrastructure.constructs.data_scraper import DataScraper\n",
    "from infrastructure.constructs.data_fetch_and_save import DataFetchAndSave\n",
    "\n",
    "\n",
    "class DataScraperStack(Stack):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: Construct,\n",
    "        construct_id: str,\n",
    "        stage_name: str,\n",
    "        service_config: dict,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(scope, construct_id, **kwargs)\n",
    "\n",
    "        DataScraper(\n",
    "            self,\n",
    "            f\"{stage_name}-DataScraper\",\n",
    "            stage_name=stage_name,\n",
    "            service_config=service_config,\n",
    "        )\n",
    "\n",
    "        DataFetchAndSave(\n",
    "            self,\n",
    "            f\"{stage_name}-DataFetchAndSave\",\n",
    "            stage_name=stage_name,\n",
    "            service_config=service_config,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "senaste versionen av lambda funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Read from DynamoDB\n",
    "    table_name = os.environ[\"BIKE_TABLE_NAME\"]\n",
    "    table = dynamodb.Table(table_name)\n",
    "\n",
    "    # Calculate timestamp for two weeks ago\n",
    "    two_weeks_ago = datetime.now() - timedelta(days=14)  # \"from today and 14 days back\"\n",
    "\n",
    "    # kan testa att ta bort hh:mm:ss\n",
    "\n",
    "    # filtering for records where timestamp is greater than the calculated 'two_weeks_ago'\n",
    "    response = table.scan(\n",
    "        FilterExpression=\"#ts > :two_weeks_ago\",\n",
    "        ExpressionAttributeNames={\"#ts\": \"timestamp\"},\n",
    "        ExpressionAttributeValues={\":two_weeks_ago\": two_weeks_ago.isoformat()},\n",
    "    )\n",
    "    # checks if there are any items in the response\n",
    "    items = response[\"Items\"] if \"Items\" in response and response[\"Items\"] else []\n",
    "\n",
    "    # if no items are found\n",
    "    if not items:\n",
    "        return {\"message\": \"No items found in DynamoDB for the last two weeks.\"}\n",
    "\n",
    "    # Convert to CSV format\n",
    "    csv_file = StringIO()  # behaves like a file but data is stored in memory\n",
    "    csv_writer = csv.DictWriter(\n",
    "        csv_file, fieldnames=items[0].keys()\n",
    "    )  # assuming all items have the same keys\n",
    "    csv_writer.writeheader()  # writes the header (column names)\n",
    "    for item in items:  # loops through the items\n",
    "        csv_writer.writerow(item)  # writes a row for each loop\n",
    "    csv_data = csv_file.getvalue()\n",
    "    csv_file.close()\n",
    "\n",
    "    # Write to S3\n",
    "    bucket_name = os.environ[\"S3_BUCKET_NAME\"]\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d%H%M%S\")  #\n",
    "    s3_key = f\"testdata_{timestamp_str}.csv\"\n",
    "    s3.put_object(Bucket=bucket_name, Key=s3_key, Body=csv_data)\n",
    "\n",
    "    return {\n",
    "        \"message\": f\"Data from the last two weeks saved to s3://{bucket_name}/{s3_key}\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import aws_cdk\n",
    " from botocore.exceptions import ClientError\n",
    " from aws_cdk import (\n",
    "     aws_dynamodb,\n",
    "     aws_lambda,\n",
    "     aws_iam,\n",
    "     Duration,\n",
    "     aws_events,\n",
    "     aws_events_targets as targets,\n",
    "     aws_s3 as s3,\n",
    " )\n",
    " from constructs import Construct\n",
    " import os\n",
    " \n",
    " \n",
    " class DataFetchAndSave(Construct):\n",
    "     def __init__(\n",
    "         self, scope: Construct, id_: str, stage_name: str, service_config: dict\n",
    "     ) -> None:\n",
    "         super().__init__(scope, id_)\n",
    " \n",
    "         service_short_name = service_config[\"service\"][\"service_short_name\"]\n",
    "         service_name = service_short_name\n",
    " \n",
    "         lambda_name = f\"{stage_name}-{service_short_name}-data-fetch-and-save\"\n",
    "         lambda_role_name = f\"{lambda_name}-role\"\n",
    "         lambda_role = self._build_lambda_role(lambda_role_name)\n",
    " \n",
    "         env_vars = self._create_env_vars(\n",
    "             stage_name=stage_name,\n",
    "             service_name=service_name,\n",
    "             service_short_name=service_short_name,\n",
    "         )\n",
    "         bucket_name = \"bikedatalake\"\n",
    " \n",
    "         self.data_fetch_and_save_lambda = self._build_lambda(\n",
    "             lambda_name=lambda_name,\n",
    "\t             stage_name=stage_name,\n",
    "             env_vars=env_vars,\n",
    "             lambda_role=lambda_role,\n",
    "         )\n",
    " \n",
    "         rule = aws_events.Rule(\n",
    "             self, \"Rule\", schedule=aws_events.Schedule.rate(Duration.days(14))\n",
    "         )\n",
    "         rule.add_target(targets.LambdaFunction(self.data_fetch_and_save_lambda))\n",
    " \n",
    "         lambda_role.add_to_policy(\n",
    "             aws_iam.PolicyStatement(\n",
    "                 actions=[\"dynamodb:Scan\", \"dynamodb:GetItem\"],\n",
    "                 resources=[\n",
    "                     \"arn:aws:dynamodb:eu-north-1:796717305864:table/Cyrille-dscrap-bike-data-table\"\n",
    "                 ],\n",
    "             )\n",
    "         )\n",
    " \n",
    "         lambda_role.add_to_policy(\n",
    "             aws_iam.PolicyStatement(\n",
    "                 actions=[\"s3:PutObject\"], resources=[f\"arn:aws:s3:::{bucket_name}\"]\n",
    "             )\n",
    "         )\n",
    " \n",
    "     def _build_lambda(\n",
    "         self,\n",
    "         lambda_name: str,\n",
    "         stage_name: str,\n",
    "         env_vars: dict,\n",
    "         lambda_role: aws_iam.Role,\n",
    "         cwd: str = os.getcwd(),\n",
    "     ):\n",
    "         lambda_function = aws_lambda.Function(\n",
    "             self,\n",
    "             lambda_name,\n",
    "             function_name=lambda_name,\n",
    "             runtime=aws_lambda.Runtime.PYTHON_3_10,\n",
    "             environment=env_vars,\n",
    "             code=aws_lambda.Code.from_asset(\n",
    "                 os.path.join(cwd, \"bike_data_scraper/handlers\")\n",
    "             ),\n",
    "             handler=\"data_fetch_and_save_lambda.lambda_handler\",\n",
    "             tracing=aws_lambda.Tracing.ACTIVE,\n",
    "             retry_attempts=2,\n",
    "             timeout=Duration.seconds(80),\n",
    "             memory_size=128,\n",
    "             role=lambda_role,\n",
    "         )\n",
    "         return lambda_function\n",
    " \n",
    "     def _build_lambda_role(self, role_name: str) -> aws_iam.Role:\n",
    "         lambda_role = aws_iam.Role(\n",
    "             self,\n",
    "             role_name,\n",
    "             assumed_by=aws_iam.ServicePrincipal(\"lambda.amazonaws.com\"),\n",
    "             managed_policies=[\n",
    "                 aws_iam.ManagedPolicy.from_aws_managed_policy_name(\n",
    "                     \"service-role/AWSLambdaBasicExecutionRole\"\n",
    "                 )\n",
    "             ],\n",
    "         )\n",
    "         return lambda_role\n",
    " \n",
    "     def _create_env_vars(\n",
    "         self, stage_name: str, service_name: str, service_short_name: str\n",
    "     ) -> dict:\n",
    "         return {\n",
    "             \"STAGE_NAME\": stage_name,\n",
    "             \"S3_BUCKET_NAME\": \"bikedatalake\",\n",
    "             \"BIKE_TABLE_NAME\": f\"{stage_name}-{service_short_name}-bike-data-table\",\n",
    "             \"LOG_LEVEL\": \"DEBUG\",\n",
    "             \"SERVICE_NAME\": service_name,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from constructs import Construct\n",
    "  \n",
    " from infrastructure.constructs.data_scraper import DataScraper\n",
    " from infrastructure.constructs.data_fetch_and_save import DataFetchAndSave\n",
    "  \n",
    "  \n",
    " class DataScraperStack(Stack):\n",
    "             stage_name=stage_name,\n",
    "             service_config=service_config,\n",
    "         )\n",
    " \n",
    "         DataFetchAndSave(\n",
    "             self,\n",
    "             f\"{stage_name}-DataFetchAndSave\",\n",
    "\t             stage_name=stage_name,\n",
    "             service_config=service_config,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 17:22:13.219039\n",
      "20231016172213\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "two_weeks_ago = datetime.now() - timedelta(days=14)\n",
    "print(two_weeks_ago)\n",
    "print(datetime.now().strftime(\"%Y%m%d%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-10-02T09:02:34.386642'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_weeks_ago.isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 10, 17, 11, 12, 4, 299775)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather-data-2023-10-17 11:14:14.049364.csv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "today = datetime.today()\n",
    "start_date = today - dt.timedelta(days=14)\n",
    "end_date = today\n",
    "    \n",
    "f\"weather-data-{today}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-10-16'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_day_ago = datetime.now() - dt.timedelta(days=1)\n",
    "one_day_ago_date = one_day_ago.date()\n",
    "two_weeks_ago = one_day_ago_date - dt.timedelta(days=14)\n",
    "\n",
    "one_day_ago_date.isoformat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-learning-Mosm6azX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
