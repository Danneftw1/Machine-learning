{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Labb/Disease_prediction/cardio_train.csv\", sep=\";\")\n",
    "\n",
    "df[\"age\"] = round(df[\"age\"]/365).astype(int) \n",
    "\n",
    "df['BMI'] = df['weight'] / (df['height']/100)**2\n",
    "\n",
    "# remove outliers that are below 15 and above 50\n",
    "df = df[df['BMI'] > 15]\n",
    "df = df[df['BMI'] < 50]\n",
    "\n",
    "df['BMI'].min(), df['BMI'].max()\n",
    "\n",
    "df['BMI_category'] = df['BMI'].apply(lambda x: 1 if x < 25 else 2 if x < 30 else 3)\n",
    "\n",
    "# removing outliers\n",
    "# set the limits for systolic blood pressure to 90-200 and for diastolic blood pressure to 60-145\n",
    "# From what I can find, you have hypotension (low blood pressure) if you go below 90/60.\n",
    "df = df[df['ap_hi'] > 90]\n",
    "df = df[df['ap_hi'] < 200] # set it to 200 since the next highest recorded value in the dataset is 197 for systolic blood pressure\n",
    "# and systolic pressure above 180 is potentially life-threatening, which means not alot of people will have a systolic blood pressure above 200\n",
    "\n",
    "# diastolic blood pressure limits\n",
    "# From what I can find, you have hypotension (low blood pressure) if you go below 60 diastolic blood pressure.\n",
    "df = df[df['ap_lo'] > 60]\n",
    "df = df[df['ap_lo'] < 145] # set the limit to 145 since the highest recorded value in the dataset is 140 for diastolic blood pressure\n",
    "\n",
    "\n",
    "df = df[df['ap_hi'] > df['ap_lo']] # removes all rows where the diastolic blood pressure is higher than the systolic blood pressure\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sys_bp = row['ap_hi']\n",
    "    dia_bp = row['ap_lo']\n",
    "\n",
    "    new_col = 'BP_category'\n",
    "\n",
    "    # Categorize the blood pressure according to the standard guidelines from wikipedia\n",
    "    if sys_bp < 120 and dia_bp < 80:\n",
    "        df.at[index, new_col] = 1\n",
    "    elif sys_bp < 130 and dia_bp < 80:\n",
    "        df.at[index, new_col] = 2\n",
    "    elif sys_bp < 140 or dia_bp < 90:\n",
    "        df.at[index, new_col] = 3\n",
    "    elif sys_bp < 180 or dia_bp < 120:\n",
    "        df.at[index, new_col] = 4\n",
    "    elif sys_bp > 180 or dia_bp > 120:\n",
    "        df.at[index, new_col] = 5\n",
    "\n",
    "df_1 = df.drop(['ap_hi', 'ap_lo', 'height', 'weight', 'BMI'], axis=1)\n",
    "df_2 = df.drop(['height', 'weight', 'BMI_category', 'BP_category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the data\n",
    "# # Drop the id column\n",
    "# df.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# # Split the dataset into train, validation, and test sets\n",
    "# X = df.drop('cardio', axis=1)\n",
    "# y = df['cardio']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# # Scale the features using feature standardization and normalization\n",
    "# scaler1 = StandardScaler()\n",
    "# scaler2 = MinMaxScaler()\n",
    "\n",
    "# X_train_scaled1 = scaler1.fit_transform(X_train)\n",
    "# X_train_scaled2 = scaler2.fit_transform(X_train)\n",
    "\n",
    "# X_val_scaled1 = scaler1.transform(X_val)\n",
    "# X_val_scaled2 = scaler2.transform(X_val)\n",
    "\n",
    "# # Define hyperparameters for each model\n",
    "# lr_param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "# dt_param_grid = {'max_depth': [None, 5, 10, 20]}\n",
    "# rf_param_grid = {'n_estimators': [100, 500, 1000]}\n",
    "# svm_param_grid = {'C': [0.01, 0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "# knn_param_grid = {'n_neighbors': [3, 5, 10, 20]}\n",
    "\n",
    "# # Train each model on the training set with different hyperparameters\n",
    "# lr = LogisticRegression()\n",
    "# dt = DecisionTreeClassifier()\n",
    "# rf = RandomForestClassifier()\n",
    "# svm = SVC()\n",
    "# knn = KNeighborsClassifier()\n",
    "\n",
    "# lr.fit(X_train_scaled1, y_train)\n",
    "# dt.fit(X_train_scaled1, y_train)\n",
    "# rf.fit(X_train_scaled1, y_train)\n",
    "# svm.fit(X_train_scaled1, y_train)\n",
    "# knn.fit(X_train_scaled1, y_train)\n",
    "\n",
    "# # Use GridSearchCV to tune the hyperparameters and choose the best model\n",
    "# lr_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data into train, validation and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_2.drop('cardio', axis=1), df_2['cardio'], test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# normalizing the features\n",
    "normalizer = MinMaxScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_val = normalizer.transform(X_val)\n",
    "X_test = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# hyperparameters for SVM\n",
    "# not chnge C\n",
    "svm_params = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'gamma': ['scale', 'auto']}\n",
    "\n",
    "# hyperparameters for RF\n",
    "rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 20, None], 'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# dictionary of hyperparameters for each model\n",
    "param_grids = {'SVM': svm_params, 'RF': rf_params}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: Best Parameters: {'gamma': 'scale', 'kernel': 'rbf'}\n",
      "SVM: Best Score: 0.7303289008900841\n",
      "RF: Best Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "RF: Best Score: 0.7328677736073337\n"
     ]
    }
   ],
   "source": [
    "# dictionary of models to train\n",
    "models = {'SVM': SVC(), 'RF': RandomForestClassifier()}\n",
    "\n",
    "# perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    grid = GridSearchCV(model, param_grid=param_grids[model_name], scoring='accuracy', cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(model_name + ': Best Parameters:', grid.best_params_)\n",
    "    print(model_name + ': Best Score:', grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.7250076585316042\n",
      "RF Validation Accuracy: 0.7281731849280098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# use best hyperparameters to make predictions on validation data\n",
    "svm = SVC(C=1, kernel='rbf', gamma='scale')\n",
    "svm.fit(X_train, y_train)\n",
    "svm_preds = svm.predict(X_val)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_val)\n",
    "\n",
    "# evaluate performance using accuracy\n",
    "svm_acc = accuracy_score(y_val, svm_preds)\n",
    "rf_acc = accuracy_score(y_val, rf_preds)\n",
    "\n",
    "print('SVM Validation Accuracy:', svm_acc)\n",
    "print('RF Validation Accuracy:', rf_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-learning-Mosm6azX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
